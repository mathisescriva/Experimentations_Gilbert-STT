"""
ExpÃ©rience 1 : Fine-tuning Whisper Large V3 sur Multilingual LibriSpeech (French)
Objectif : CrÃ©er gilbert-whisper-l3-fr-base-v1
"""

import modal

image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("ffmpeg")  # NÃ©cessaire pour torchcodec
    .pip_install(
        "torch>=2.0.0",
        "transformers>=4.36.0",
        "datasets>=2.14.0",
        "accelerate>=0.25.0",
        "librosa>=0.10.0",
        "soundfile>=0.12.0",
        "evaluate>=0.4.0",
        "jiwer>=3.0.0",  # NÃ©cessaire pour la mÃ©trique WER - VERSION EXPLICITE
        "numpy>=1.24.0",
        "tqdm>=4.65.0",
        "tensorboard>=2.14.0",
        "huggingface-hub>=0.19.0",
        "hf_transfer",
        "torchcodec",  # NÃ©cessaire pour dÃ©coder l'audio dans datasets
    )
    .run_commands("pip install --upgrade jiwer")  # Force l'installation de jiwer
)

app = modal.App("whisper-exp1-fr-base")


@app.function(
    image=image,
    gpu="H200",  # H200 est plus rapide, remis comme demandÃ©
    timeout=86400,  # 24 heures
    volumes={
        "/model_cache": modal.Volume.from_name("whisper-models", create_if_missing=True),
        "/output": modal.Volume.from_name("whisper-output", create_if_missing=True),
        "/preprocessed_data": modal.Volume.from_name("whisper-preprocessed", create_if_missing=True),
    },
)
def train_whisper():
    """Fonction d'entraÃ®nement - ExpÃ©rience 1"""
    import os
    import torch
    from datasets import load_dataset, Audio
    from transformers import (
        WhisperProcessor,
        WhisperFeatureExtractor,  # AjoutÃ© pour charger sÃ©parÃ©ment
        WhisperTokenizer,  # AjoutÃ© pour charger sÃ©parÃ©ment
        WhisperForConditionalGeneration,
        Seq2SeqTrainingArguments,
        Seq2SeqTrainer,
    )
    from transformers.models.whisper.english_normalizer import BasicTextNormalizer
    import evaluate
    from dataclasses import dataclass
    from typing import Any, Dict, List, Union
    import numpy as np

    # Configuration ExpÃ©rience 1
    MODEL_NAME = "openai/whisper-large-v3"
    OUTPUT_DIR = "/output/gilbert-whisper-l3-fr-base-v1"
    SAMPLING_RATE = 16000
    TRAIN_TEST_SPLIT = 0.95

    # Dataset : Multilingual LibriSpeech French uniquement
    DATASET_NAME = "facebook/multilingual_librispeech"
    DATASET_CONFIG = "french"
    TEXT_COLUMN = "transcript"  # La colonne s'appelle "transcript" dans ce dataset
    AUDIO_COLUMN = "audio"

    @dataclass
    class DataCollatorSpeechSeq2SeqWithPadding:
        """
        Data collator that pads audio features and text labels for batch training.
        BasÃ© sur l'exemple officiel Modal.
        """
        processor: WhisperProcessor
        decoder_start_token_id: int  # AjoutÃ© comme dans l'exemple officiel

        def __call__(
            self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
        ) -> Dict[str, torch.Tensor]:
            # SÃ©parer les features audio et les labels texte (besoin de padding diffÃ©rent)
            model_input_name = self.processor.model_input_names[0]  # "input_features" pour Whisper
            input_features = [
                {model_input_name: feature[model_input_name]} for feature in features
            ]
            label_features = [{"input_ids": feature["labels"]} for feature in features]

            # Pad les features audio
            batch = self.processor.feature_extractor.pad(
                input_features,
                return_tensors="pt",
                return_attention_mask=True,  # AjoutÃ© comme dans l'exemple officiel
                padding=True,  # AjoutÃ© comme dans l'exemple officiel
            )

            # Pad les labels texte
            labels_batch = self.processor.tokenizer.pad(
                label_features, return_tensors="pt"
            )
            
            # Remplacer les tokens de padding par -100 pour qu'ils soient ignorÃ©s dans le calcul de loss
            labels = labels_batch["input_ids"].masked_fill(
                labels_batch.attention_mask.ne(1), -100
            )

            # IMPORTANT: Retirer le start token si le tokenizer l'a ajoutÃ©
            # Le modÃ¨le l'ajoutera automatiquement pendant l'entraÃ®nement
            if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
                labels = labels[:, 1:]

            batch["labels"] = labels
            return batch

    def load_and_prepare_datasets():
        """Charge Multilingual LibriSpeech French"""
        print("=" * 60)
        print("ğŸ“š Dataset: Multilingual LibriSpeech (French)")
        print("=" * 60)
        
        print(f"\nğŸ“¦ Loading {DATASET_NAME} (config: {DATASET_CONFIG})...")
        
        try:
            # Charger le dataset
            dataset = load_dataset(
                DATASET_NAME,
                DATASET_CONFIG,
                split="train",
            )
            
            print(f"   âœ“ Loaded: {len(dataset)} samples")
            print(f"   ğŸ“‹ Columns: {dataset.column_names}")
            
            # VÃ©rifier les colonnes
            if AUDIO_COLUMN not in dataset.column_names:
                raise ValueError(f"Audio column '{AUDIO_COLUMN}' not found in dataset")
            if TEXT_COLUMN not in dataset.column_names:
                raise ValueError(f"Text column '{TEXT_COLUMN}' not found in dataset")
            
            # Caster l'audio en 16kHz mono
            print(f"\nğŸµ Casting audio to {SAMPLING_RATE}Hz mono...")
            dataset = dataset.cast_column(
                AUDIO_COLUMN,
                Audio(sampling_rate=SAMPLING_RATE)
            )
            
            # SÃ©lectionner et renommer les colonnes
            dataset = dataset.select_columns([AUDIO_COLUMN, TEXT_COLUMN])
            dataset = dataset.rename_columns({
                AUDIO_COLUMN: "audio",
                TEXT_COLUMN: "text"
            })
            
            # Filtrer les textes vides
            dataset = dataset.filter(
                lambda x: x["text"] is not None and len(str(x["text"]).strip()) > 0
            )
            
            print(f"   âœ“ Final dataset size: {len(dataset)} samples")
            
            # Shuffle et split
            dataset = dataset.shuffle(seed=42)
            split_dataset = dataset.train_test_split(
                test_size=1 - TRAIN_TEST_SPLIT,
                seed=42
            )
            
            print(f"\nğŸ“Š Train/Test split:")
            print(f"   âœ“ Train: {len(split_dataset['train'])} samples")
            print(f"   âœ“ Test: {len(split_dataset['test'])} samples")
            
            return split_dataset
            
        except Exception as e:
            print(f"   âŒ Error loading dataset: {e}")
            import traceback
            traceback.print_exc()
            raise

    def prepare_dataset(batch, feature_extractor, tokenizer, model_input_name):
        """PrÃ©pare un batch d'exemples pour l'entraÃ®nement (BATCHED VERSION)"""
        # Version batchÃ©e comme dans l'exemple officiel Modal
        # IMPORTANT: Utiliser feature_extractor et tokenizer sÃ©parÃ©ment (pas processor)
        audio_arrays = [item["array"] for item in batch["audio"]]
        
        # Extraire les features audio en batch
        inputs = feature_extractor(
            audio_arrays,
            sampling_rate=feature_extractor.sampling_rate,
        )
        batch[model_input_name] = inputs.get(model_input_name)  # Utiliser model_input_name dynamique
        
        # Tokenizer les textes en batch
        batch["labels"] = tokenizer(batch["text"]).input_ids
        
        # Calculer la longueur pour group_by_length
        batch["input_length"] = [len(arr) for arr in audio_arrays]
        
        return batch

    def compute_metrics(pred, tokenizer, normalizer, metric):
        """Calcule le WER - comme l'exemple officiel Modal"""
        pred_ids = pred.predictions
        label_ids = pred.label_ids
        
        # Remplacer -100 par pad_token_id pour le dÃ©codage
        label_ids[label_ids == -100] = tokenizer.pad_token_id
        
        # DÃ©coder
        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
        
        # Normaliser
        pred_str = [normalizer(pred).strip() for pred in pred_str]
        label_str = [normalizer(label).strip() for label in label_str]
        
        # Calculer WER
        wer = metric.compute(predictions=pred_str, references=label_str)
        
        return {"wer": wer}

    # ========== MAIN TRAINING ==========
    print("=" * 60)
    print("ğŸš€ EXPÃ‰RIENCE 1 : Fine-tuning Whisper Large V3")
    print("ğŸ“š Dataset: Multilingual LibriSpeech (French)")
    print("ğŸ¯ Objectif: gilbert-whisper-l3-fr-base-v1")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ“± Device: {device}")
    
    # DÃ©sactiver hf_transfer si problÃ¨me
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
    
    # Charger le modÃ¨le - IMPORTANT: Charger feature_extractor et tokenizer SÃ‰PARÃ‰MENT (comme l'exemple officiel)
    print(f"\nğŸ“¥ Loading model: {MODEL_NAME}...")
    feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL_NAME, cache_dir="/model_cache")
    tokenizer = WhisperTokenizer.from_pretrained(MODEL_NAME, cache_dir="/model_cache")
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir="/model_cache")
    
    # Configuration importante : dÃ©sactiver forced_decoder_ids et suppress_tokens
    model.config.forced_decoder_ids = None
    model.config.suppress_tokens = []
    
    print("   âœ“ Model loaded")
    
    # CrÃ©er le processor APRÃˆS (comme l'exemple officiel)
    processor = WhisperProcessor(
        feature_extractor=feature_extractor,
        tokenizer=tokenizer,
    )
    
    # Charger et prÃ©parer les datasets
    datasets = load_and_prepare_datasets()
    
    # PrÃ©parer les datasets pour l'entraÃ®nement
    print("\nğŸ”§ Preparing datasets for training...")
    
    # VÃ©rifier si le preprocessing est dÃ©jÃ  sauvegardÃ©
    preprocessed_train_path = "/preprocessed_data/train_dataset"
    preprocessed_test_path = "/preprocessed_data/test_dataset"
    
    import os
    if os.path.exists(preprocessed_train_path) and os.path.exists(preprocessed_test_path):
        print("   ğŸ“¦ Chargement du preprocessing sauvegardÃ©...")
        from datasets import load_from_disk
        train_dataset = load_from_disk(preprocessed_train_path)
        test_dataset = load_from_disk(preprocessed_test_path)
        print("   âœ“ Datasets prÃ©processÃ©s chargÃ©s depuis le cache")
    else:
        print("   ğŸ”„ Preprocessing des datasets (premiÃ¨re fois)...")
        
        # Utiliser batched=True comme dans l'exemple officiel Modal
        # IMPORTANT: Passer feature_extractor et tokenizer sÃ©parÃ©ment (pas processor)
        import functools
        import os
        model_input_name = feature_extractor.model_input_names[0]  # "input_features" pour Whisper
        prepare_fn = functools.partial(
            prepare_dataset,
            feature_extractor=feature_extractor,
            tokenizer=tokenizer,
            model_input_name=model_input_name,
        )
        
        train_dataset = datasets["train"].map(
            prepare_fn,
            batched=True,  # CRUCIAL: traite par batch au lieu d'un par un
            remove_columns=datasets["train"].column_names,
            num_proc=os.cpu_count(),  # Comme l'exemple officiel
            desc="Feature extract + tokenize (train)",
        )
        
        test_dataset = datasets["test"].map(
            prepare_fn,
            batched=True,  # CRUCIAL: traite par batch au lieu d'un par un
            remove_columns=datasets["test"].column_names,
            num_proc=os.cpu_count(),  # Comme l'exemple officiel
            desc="Feature extract + tokenize (test)",
        )
        
        # Sauvegarder pour la prochaine fois
        print("   ğŸ’¾ Sauvegarde du preprocessing...")
        train_dataset.save_to_disk(preprocessed_train_path)
        test_dataset.save_to_disk(preprocessed_test_path)
        print("   âœ“ Datasets prÃ©parÃ©s et sauvegardÃ©s")
    
    # Data collator et mÃ©trique
    # IMPORTANT: Passer decoder_start_token_id comme dans l'exemple officiel Modal
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(
        processor=processor,
        decoder_start_token_id=model.config.decoder_start_token_id
    )
    
    # VÃ©rifier que jiwer est installÃ© avant de charger la mÃ©trique
    try:
        import jiwer
        print("   âœ“ jiwer installÃ©")
    except ImportError:
        print("   âš ï¸  jiwer non trouvÃ©, installation...")
        import subprocess
        subprocess.check_call(["pip", "install", "jiwer"])
        import jiwer
    
    wer_metric = evaluate.load("wer")
    
    # Arguments d'entraÃ®nement - OPTIMISÃ‰S POUR VITESSE MAXIMALE (H200)
    # BasÃ© sur l'exemple officiel Modal
    # Gestion de la compatibilitÃ© eval_strategy vs evaluation_strategy
    import transformers
    transformers_version = transformers.__version__
    print(f"   ğŸ“¦ Transformers version: {transformers_version}")
    
    # DÃ©terminer quel paramÃ¨tre utiliser selon la version
    # Dans transformers >= 4.37, eval_strategy remplace evaluation_strategy
    try:
        # Essayer d'abord avec eval_strategy (versions rÃ©centes)
        training_args_dict = {
            "output_dir": OUTPUT_DIR,
            "per_device_train_batch_size": 24,  # H200 a 141GB de mÃ©moire
            "per_device_eval_batch_size": 24,
            "gradient_accumulation_steps": 1,
            "learning_rate": 1e-5,
            "num_train_epochs": 1,
            "fp16": True,  # Comme l'exemple officiel Modal (bf16 est mieux pour H100/H200 mais fp16 est plus compatible)
            # Pas de dataloader_num_workers explicite (comme l'exemple officiel Modal)
            # "dataloader_pin_memory": True,  # RetirÃ© pour correspondre Ã  l'exemple officiel
            "eval_strategy": "steps",  # ParamÃ¨tre moderne (transformers >= 4.37)
            "eval_steps": 5000,
            "save_strategy": "steps",
            "save_steps": 5000,
            "logging_steps": 100,
            "report_to": "tensorboard",
            "load_best_model_at_end": True,
            "metric_for_best_model": "wer",
            "greater_is_better": False,
            "predict_with_generate": True,
            "generation_max_length": 225,
            "generation_num_beams": 1,  # Comme dans l'exemple officiel
            "save_total_limit": 3,
            "push_to_hub": False,
            "gradient_checkpointing": False,  # H200 a largement assez de mÃ©moire
            # "bf16": True,  # DÃ©sactivÃ© - utiliser fp16 comme l'exemple officiel (plus compatible)
            "group_by_length": True,  # RÃ©activÃ© - comme l'exemple officiel (avec toutes les corrections, Ã§a devrait fonctionner)
            "length_column_name": "input_length",  # Colonne crÃ©Ã©e dans prepare_dataset (pas utilisÃ©e si group_by_length=False)
        }
        training_args = Seq2SeqTrainingArguments(**training_args_dict)
    except TypeError as e:
        if "eval_strategy" in str(e):
            # Fallback vers evaluation_strategy pour versions anciennes
            print("   âš ï¸  eval_strategy non supportÃ©, utilisation de evaluation_strategy")
            training_args_dict["evaluation_strategy"] = training_args_dict.pop("eval_strategy")
            training_args = Seq2SeqTrainingArguments(**training_args_dict)
        else:
            raise
    
    # CrÃ©er normalizer comme dans l'exemple officiel
    normalizer = BasicTextNormalizer()
    
    def compute_metrics_fn(pred):
        return compute_metrics(pred, tokenizer, normalizer, wer_metric)
    
    # Pas besoin de gradient checkpointing avec H200 (141GB de mÃ©moire)
    # model.gradient_checkpointing_enable()  # DÃ©sactivÃ© pour H200
    
    # CrÃ©er le trainer
    # IMPORTANT: Utiliser processing_class au lieu de tokenizer (dÃ©prÃ©ciÃ©)
    # Comme dans l'exemple officiel Modal
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        processing_class=feature_extractor,  # IMPORTANT: Utiliser feature_extractor directement (pas processor.feature_extractor)
        data_collator=data_collator,
        compute_metrics=compute_metrics_fn,
    )
    
    # VÃ©rifier s'il existe un checkpoint pour reprendre l'entraÃ®nement
    import glob
    checkpoint_dirs = sorted(glob.glob(f"{OUTPUT_DIR}/checkpoint-*"))
    resume_from_checkpoint = None
    
    if checkpoint_dirs:
        # Prendre le dernier checkpoint (le plus rÃ©cent)
        resume_from_checkpoint = checkpoint_dirs[-1]
        print(f"\nğŸ”„ Checkpoint trouvÃ© : {resume_from_checkpoint}")
        print("   Reprise de l'entraÃ®nement depuis le checkpoint...")
    else:
        print("\nğŸ†• Aucun checkpoint trouvÃ©, dÃ©marrage depuis le dÃ©but")
    
    # Lancer l'entraÃ®nement
    print("\n" + "=" * 60)
    print("ğŸ‹ï¸  Starting training...")
    print("=" * 60)
    
    # Logs de debug avant l'entraÃ®nement
    print(f"   ğŸ“Š Train dataset size: {len(train_dataset)}")
    print(f"   ğŸ“Š Eval dataset size: {len(test_dataset)}")
    print(f"   ğŸ¯ Output dir: {OUTPUT_DIR}")
    print(f"   ğŸ’¾ GPU available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   ğŸ® GPU name: {torch.cuda.get_device_name(0)}")
        print(f"   ğŸ’¾ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print("   â³ Calling trainer.train()...")
    import sys
    sys.stdout.flush()  # Force l'affichage
    
    # Ajouter un timeout et des logs supplÃ©mentaires
    print("   ğŸ”„ Initialisation du DataLoader et chargement du premier batch...")
    sys.stdout.flush()
    
    # IMPORTANT: Faire une Ã©valuation baseline AVANT l'entraÃ®nement (comme l'exemple officiel Modal)
    # Cela initialise le trainer et peut Ã©viter les blocages
    print("\nğŸ“Š Running baseline evaluation (initializes trainer)...")
    sys.stdout.flush()
    try:
        baseline_metrics = trainer.evaluate(
            metric_key_prefix="baseline",
            max_length=training_args.generation_max_length,
            num_beams=training_args.generation_num_beams,
        )
        trainer.log_metrics("baseline", baseline_metrics)
        trainer.save_metrics("baseline", baseline_metrics)
        print(f"   âœ“ Baseline WER: {baseline_metrics.get('baseline_wer', 'N/A')}")
        sys.stdout.flush()
    except Exception as e:
        print(f"   âš ï¸  Baseline eval failed (continuing anyway): {e}")
        sys.stdout.flush()
    
    # Maintenant lancer l'entraÃ®nement
    if resume_from_checkpoint:
        print(f"\nğŸ”„ Resuming from: {resume_from_checkpoint}")
        sys.stdout.flush()
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    else:
        print("\nğŸ†• Starting training from scratch...")
        sys.stdout.flush()
        print(f"   ğŸ“ Weights will be saved to '{training_args.output_dir}'")
        sys.stdout.flush()
        trainer.train()
    
    print("   âœ… trainer.train() completed!")
    
    # Sauvegarder le modÃ¨le
    print(f"\nğŸ’¾ Saving model to {OUTPUT_DIR}...")
    trainer.save_model()
    processor.save_pretrained(OUTPUT_DIR)
    
    print("\nâœ… Training completed!")
    print(f"ğŸ“ Model saved to: {OUTPUT_DIR}")
    print(f"ğŸ¯ Model name: gilbert-whisper-l3-fr-base-v1")
    
    # Ã‰valuation finale
    print("\nğŸ“Š Running final evaluation...")
    eval_results = trainer.evaluate()
    print(f"   Final WER: {eval_results.get('eval_wer', 'N/A')}")
    
    return {
        "wer": eval_results.get('eval_wer', 'N/A'),
        "output_dir": OUTPUT_DIR,
        "model_name": "gilbert-whisper-l3-fr-base-v1"
    }


@app.local_entrypoint()
def main():
    """Point d'entrÃ©e local"""
    print("ğŸš€ Lancement de l'ExpÃ©rience 1 sur Modal...")
    print("ğŸ“š Dataset: Multilingual LibriSpeech (French)")
    print("ğŸ¯ Objectif: gilbert-whisper-l3-fr-base-v1")
    print("=" * 60)
    
    try:
        result = train_whisper.remote()
        print(f"\nâœ… EntraÃ®nement terminÃ© !")
        print(f"ğŸ“Š WER final: {result['wer']}")
        print(f"ğŸ“ ModÃ¨le sauvegardÃ© dans: {result['output_dir']}")
        print(f"ğŸ¯ Nom du modÃ¨le: {result['model_name']}")
    except Exception as e:
        print(f"\nâŒ Erreur: {e}")
        import traceback
        traceback.print_exc()

