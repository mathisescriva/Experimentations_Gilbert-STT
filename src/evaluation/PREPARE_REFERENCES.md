# Guide : Pr√©parer les R√©f√©rences pour le Benchmark

Ce guide explique comment obtenir automatiquement les transcriptions de r√©f√©rence sans avoir √† les transcrire manuellement.

## üéØ M√©thodes Disponibles

### 1. üìö Depuis des Datasets HuggingFace (RECOMMAND√â)

**Meilleure option** : Utiliser des datasets publics qui ont d√©j√† des transcriptions de haute qualit√©.

#### Exemple : Multilingual LibriSpeech (Fran√ßais)

```bash
python -m src.evaluation.prepare_references dataset \
  --dataset "facebook/multilingual_librispeech" \
  --config "french" \
  --audio-column "audio" \
  --text-column "transcript" \
  --subset "meetings" \
  --limit 100
```

Cela va :
- T√©l√©charger le dataset depuis HuggingFace
- Extraire les fichiers audio et leurs transcriptions
- Les sauvegarder dans `benchmark/meetings/audio/` et `benchmark/meetings/refs/`

#### Autres Datasets Utiles

**Pour les r√©unions :**
```bash
# VoxPopuli (discours longs, proches de r√©unions)
python -m src.evaluation.prepare_references dataset \
  --dataset "facebook/voxpopuli" \
  --config "fr" \
  --subset "meetings"
```

**Pour les accents r√©gionaux :**
```bash
# Common Voice (diversit√© d'accents)
python -m src.evaluation.prepare_references dataset \
  --dataset "mozilla-foundation/common_voice_17_0" \
  --config "fr" \
  --subset "accents"
```

### 2. üé¨ Depuis des Fichiers de Sous-titres

Si vous avez des fichiers audio avec des sous-titres (.srt ou .vtt) :

```bash
# Installer les d√©pendances pour les sous-titres
pip install pysrt webvtt

# Extraire les transcriptions
python -m src.evaluation.prepare_references subtitles \
  --subtitle-dir /path/to/subtitles \
  --audio-dir benchmark/meetings/audio \
  --output-refs-dir benchmark/meetings/refs \
  --format srt
```

**Format attendu :**
- Fichiers audio : `meeting_001.wav`
- Fichiers sous-titres : `meeting_001.srt` (m√™me nom de base)

### 3. ü§ñ Utiliser un Mod√®le de R√©f√©rence (Pseudo-r√©f√©rences)

‚ö†Ô∏è **ATTENTION** : Cette m√©thode g√©n√®re des "pseudo-r√©f√©rences" (pas de vraies r√©f√©rences). 
Utilisez un mod√®le de tr√®s haute qualit√© (Whisper Large V3) et **v√©rifiez manuellement** les r√©sultats.

```bash
# G√©n√©rer des pseudo-r√©f√©rences avec Whisper Large V3
python -m src.evaluation.prepare_references model \
  --audio-dir benchmark/meetings/audio \
  --output-refs-dir benchmark/meetings/refs \
  --model-name "openai/whisper-large-v3" \
  --device cuda
```

**Quand utiliser cette m√©thode :**
- Pour un setup rapide et tester le pipeline
- Si vous avez des fichiers audio sans transcriptions
- **MAIS** : V√©rifiez et corrigez manuellement avant d'utiliser pour l'√©valuation finale

### 4. üìÑ Depuis des Fichiers Texte Existants

Si vous avez d√©j√† des fichiers texte avec les transcriptions :

```bash
python -m src.evaluation.prepare_references text \
  --text-dir /path/to/text/files \
  --output-refs-dir benchmark/meetings/refs \
  --audio-dir benchmark/meetings/audio  # Optionnel : v√©rifie que l'audio existe
```

## üöÄ Workflow Recommand√©

### Option A : Datasets Publics (Id√©al)

1. **Choisir un dataset appropri√©** selon le type de contenu
2. **Extraire les donn√©es** :
   ```bash
   python -m src.evaluation.prepare_references dataset \
     --dataset "facebook/multilingual_librispeech" \
     --config "french" \
     --subset "meetings" \
     --limit 50
   ```
3. **V√©rifier quelques √©chantillons** manuellement
4. **Lancer le benchmark** !

### Option B : Vos Propres Donn√©es

1. **Placer vos fichiers audio** dans `benchmark/{subset}/audio/`
2. **G√©n√©rer des pseudo-r√©f√©rences** :
   ```bash
   python -m src.evaluation.prepare_references model \
     --audio-dir benchmark/meetings/audio \
     --output-refs-dir benchmark/meetings/refs \
     --model-name "openai/whisper-large-v3"
   ```
3. **V√©rifier et corriger** les transcriptions g√©n√©r√©es
4. **Lancer le benchmark**

### Option C : Sous-titres Existants

1. **Placer audio + sous-titres** dans des dossiers s√©par√©s
2. **Extraire les transcriptions** :
   ```bash
   python -m src.evaluation.prepare_references subtitles \
     --subtitle-dir /path/to/subtitles \
     --audio-dir benchmark/meetings/audio \
     --output-refs-dir benchmark/meetings/refs
   ```
3. **V√©rifier** que les noms correspondent
4. **Lancer le benchmark**

## üìã Checklist

Avant de lancer le benchmark, v√©rifiez :

- [ ] Les fichiers audio sont dans `benchmark/{subset}/audio/`
- [ ] Les fichiers de r√©f√©rence sont dans `benchmark/{subset}/refs/`
- [ ] Les noms correspondent (ex: `audio.wav` ‚Üí `audio.txt`)
- [ ] Les r√©f√©rences sont en UTF-8
- [ ] Vous avez v√©rifi√© quelques √©chantillons manuellement

## üí° Astuces

1. **Commencez petit** : Testez avec 10-20 √©chantillons d'abord
2. **V√©rifiez la qualit√©** : Regardez quelques transcriptions g√©n√©r√©es
3. **M√©langez les sources** : Utilisez diff√©rents datasets pour diff√©rents sous-ensembles
4. **Conservez les originaux** : Gardez une copie de vos donn√©es brutes

## üîß D√©pendances Optionnelles

Pour les sous-titres :
```bash
pip install pysrt webvtt
```

Ces d√©pendances ne sont pas dans `requirements.txt` car elles sont optionnelles.

