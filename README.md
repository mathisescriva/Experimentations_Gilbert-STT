# Fine-tuning Whisper Large V3 pour le FranÃ§ais

Ce projet permet de fine-tuner le modÃ¨le `openai/whisper-large-v3` pour amÃ©liorer les performances en franÃ§ais et en contexte de rÃ©unions/discussions longues.

## ğŸ¯ Objectif

AmÃ©liorer progressivement Whisper Large V3 pour :
- âœ… AmÃ©liorer la transcription en franÃ§ais
- âœ… AmÃ©liorer la robustesse en rÃ©unions/discussions longues
- âœ… Conserver les capacitÃ©s multilingues

## ğŸ“š Datasets utilisÃ©s

Le script utilise actuellement des datasets publics :

1. **fsicoli/common_voice_17_0** (FR)
   - DiversitÃ© d'accents et conditions rÃ©elles
   - Colonne texte : `sentence`

2. **facebook/voxpopuli** (FR)
   - Discours longs, oratoires, proches de rÃ©unions
   - Colonne texte : `normalized_text` ou `raw_text` (dÃ©tection automatique)

3. **diarizers-community/ami** (optionnel, pour plus tard)
   - Corpus de rÃ©unions (anglais), speech spontanÃ© & overlaps
   - Ã€ intÃ©grer plus tard pour "apprendre le style rÃ©union"

## ğŸ“¦ Installation

### PrÃ©requis

- Python 3.8+
- CUDA (pour GPU cloud) ou Metal (pour Mac M3 Pro)

### Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

## ğŸ’» Utilisation

### ğŸš« Limitations sur MacBook Pro M3 Pro

**Ã€ NE PAS faire en local :**
- âŒ Fine-tuner Whisper Large V3 complet â†’ trop lourd, GPU Metal insuffisant
- âŒ EntraÃ®nement multi-epoch sur datasets FR complets â†’ trop lent

**Ce qui est faisable en local :**
- âœ… DÃ©veloppement et test du code
- âœ… TÃ©lÃ©chargement/preprocessing des datasets
- âœ… VÃ©rification du pipeline avec un mini-training (quelques batches)
- âœ… Fine-tuning lÃ©ger/LoRA sur Whisper-small/medium si besoin
- âœ… Tests d'infÃ©rence

### ğŸ–¥ï¸ DÃ©veloppement local (Mac M3 Pro)

#### 1. Tester le chargement des datasets

Vous pouvez modifier temporairement le script pour tester avec un sous-Ã©chantillon :

```python
# Dans train_whisper_fr.py, ajouter aprÃ¨s le chargement :
dataset = dataset.select(range(100))  # Test avec 100 Ã©chantillons
```

#### 2. Mini-training de test

Pour vÃ©rifier que le pipeline fonctionne, vous pouvez rÃ©duire les paramÃ¨tres :

```bash
# Modifier dans train_whisper_fr.py :
# - num_train_epochs = 0.1  # Juste quelques batches
# - max_steps = 10  # Limiter Ã  10 steps
# - per_device_train_batch_size = 1  # Batch size minimal

python train_whisper_fr.py
```

âš ï¸ **Note** : MÃªme avec ces rÃ©ductions, le training complet sera trÃ¨s lent sur Mac M3 Pro. Utilisez cela uniquement pour valider que le code fonctionne.

### â˜ï¸ Training sur GPU Cloud (A100)

#### Option 1 : RunPod

1. **CrÃ©er une instance RunPod**
   - Template : PyTorch
   - GPU : A100 40GB ou 80GB
   - OS : Ubuntu 22.04

2. **Se connecter et cloner le projet**

```bash
# Sur votre Mac
scp -r . user@runpod-ip:/workspace/whisper-finetuning/

# Ou cloner depuis Git
git clone <votre-repo> /workspace/whisper-finetuning
cd /workspace/whisper-finetuning
```

3. **Installer les dÃ©pendances**

```bash
pip install -r requirements.txt
```

4. **Lancer l'entraÃ®nement**

```bash
# Avec accelerate (recommandÃ© pour multi-GPU)
accelerate config  # Configurer une fois
accelerate launch train_whisper_fr.py

# Ou directement
python train_whisper_fr.py
```

#### Option 2 : Lambda Labs

1. **CrÃ©er une instance Lambda Labs**
   - GPU : A100 40GB ou 80GB

2. **MÃªme procÃ©dure que RunPod**

#### Option 3 : HuggingFace Spaces / Inference Endpoints

Pour un setup plus simple, vous pouvez utiliser les ressources HuggingFace.

### ğŸ“¥ RÃ©cupÃ©rer les poids finetunÃ©s

Une fois l'entraÃ®nement terminÃ© sur le cloud :

```bash
# Depuis le serveur cloud
# Option 1 : TÃ©lÃ©charger via SCP
scp -r user@cloud-ip:/workspace/whisper-finetuning/gilbert-whisper-large-v3-fr-v1 ./

# Option 2 : Upload vers HuggingFace Hub (si configurÃ©)
# Dans train_whisper_fr.py, activer push_to_hub=True
```

## ğŸ”§ Configuration

### ParamÃ¨tres d'entraÃ®nement

Les paramÃ¨tres par dÃ©faut dans `train_whisper_fr.py` :

- `learning_rate`: 1e-5
- `per_device_train_batch_size`: 4
- `gradient_accumulation_steps`: 4
- `num_train_epochs`: 1 (phase test)
- `fp16`: True (mixed precision)
- `predict_with_generate`: True

### Modifier les paramÃ¨tres

Ã‰ditez directement `train_whisper_fr.py` ou utilisez des variables d'environnement :

```bash
export LEARNING_RATE=1e-5
export BATCH_SIZE=4
python train_whisper_fr.py
```

## ğŸ§ª InfÃ©rence avec le modÃ¨le finetunÃ©

### Utilisation du script d'exemple

Un script d'infÃ©rence est fourni pour faciliter la transcription :

```bash
python inference_example.py path/to/audio.wav
```

Ou avec un modÃ¨le personnalisÃ© :

```bash
python inference_example.py path/to/audio.wav --model-path ./gilbert-whisper-large-v3-fr-v1
```

### Utilisation en Python

Vous pouvez aussi utiliser le modÃ¨le directement dans votre code :

```python
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torchaudio

# Charger le modÃ¨le finetunÃ©
processor = WhisperProcessor.from_pretrained("./gilbert-whisper-large-v3-fr-v1")
model = WhisperForConditionalGeneration.from_pretrained("./gilbert-whisper-large-v3-fr-v1")

# Charger l'audio
audio_path = "path/to/audio.wav"
audio, sr = torchaudio.load(audio_path)
audio = audio.squeeze().numpy()

# PrÃ©traiter
inputs = processor(audio, sampling_rate=16000, return_tensors="pt")

# GÃ©nÃ©rer
with torch.no_grad():
    generated_ids = model.generate(inputs["input_features"])

# DÃ©coder
transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(transcription)
```

## ğŸ“Š Monitoring

Le script utilise TensorBoard pour le monitoring. Pour visualiser :

```bash
tensorboard --logdir ./gilbert-whisper-large-v3-fr-v1/runs
```

## ğŸ› DÃ©pannage

### Erreur de mÃ©moire GPU

RÃ©duire `per_device_train_batch_size` ou augmenter `gradient_accumulation_steps` :

```python
per_device_train_batch_size = 2
gradient_accumulation_steps = 8
```

### Datasets trop volumineux

Utiliser le streaming :

```python
dataset = load_dataset(..., streaming=True)
```

### ProblÃ¨mes de tÃ©lÃ©chargement

Les datasets sont tÃ©lÃ©chargÃ©s automatiquement au premier lancement. VÃ©rifiez votre connexion internet et l'espace disque disponible.

## ğŸ“Š Benchmark

Ce projet inclut un systÃ¨me de benchmark interne pour Ã©valuer les performances ASR sur diffÃ©rents types de contenu.

### Structure du benchmark

Le benchmark est organisÃ© en sous-ensembles dans le dossier `benchmark/` :

```
benchmark/
â”œâ”€â”€ meetings/
â”‚   â”œâ”€â”€ audio/     # Fichiers audio de rÃ©unions
â”‚   â””â”€â”€ refs/      # Textes de rÃ©fÃ©rence (.txt)
â”œâ”€â”€ telephone/
â”‚   â”œâ”€â”€ audio/     # Fichiers audio tÃ©lÃ©phoniques
â”‚   â””â”€â”€ refs/
â”œâ”€â”€ accents/
â”‚   â”œâ”€â”€ audio/      # Fichiers avec accents rÃ©gionaux
â”‚   â””â”€â”€ refs/
â””â”€â”€ longform/
    â”œâ”€â”€ audio/      # Contenu long format
    â””â”€â”€ refs/
```

Chaque fichier audio doit avoir un fichier de rÃ©fÃ©rence correspondant dans `refs/` avec le mÃªme nom (ex: `audio/sample.wav` â†’ `refs/sample.txt`).

### Utilisation

1. **PrÃ©parer les donnÃ©es** : Placez vos fichiers audio et leurs rÃ©fÃ©rences dans les dossiers appropriÃ©s.

2. **Configurer le benchmark** : Ã‰ditez `configs/benchmark.yaml` :
   ```yaml
   model_name: "MEscriva/gilbert-fr-source"  # Votre modÃ¨le
   device: "cuda"  # ou "cpu"
   subsets: ["meetings", "telephone", "accents", "longform"]
   ```

3. **Lancer le benchmark** :
   ```bash
   python -m src.evaluation.run_benchmark --config configs/benchmark.yaml
   ```

   Ou avec des options personnalisÃ©es :
   ```bash
   python -m src.evaluation.run_benchmark \
     --config configs/benchmark.yaml \
     --model-name "MEscriva/gilbert-fr-source" \
     --compute-cer \
     --output-csv results.csv
   ```

### MÃ©triques

Le benchmark calcule actuellement :
- **WER (Word Error Rate)** : Taux d'erreur au niveau des mots
- **CER (Character Error Rate)** : Taux d'erreur au niveau des caractÃ¨res (optionnel avec `--compute-cer`)

Les rÃ©sultats sont affichÃ©s dans un tableau rÃ©capitulatif par sous-ensemble et une moyenne globale.

## ğŸ“ Structure du projet

```
.
â”œâ”€â”€ train_whisper_exp1.py      # Script principal d'entraÃ®nement (Modal)
â”œâ”€â”€ evaluate_exp1.py            # Script d'Ã©valuation aprÃ¨s entraÃ®nement
â”œâ”€â”€ inference_example.py        # Script d'exemple pour l'infÃ©rence
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ README.md                   # Ce fichier
â”œâ”€â”€ .gitignore                  # Fichiers Ã  ignorer par Git
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ benchmark.yaml         # Configuration du benchmark
â”œâ”€â”€ src/
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ run_benchmark.py    # Script CLI pour le benchmark
â”‚       â”œâ”€â”€ asr_inference.py    # Wrapper pour l'infÃ©rence ASR
â”‚       â””â”€â”€ metrics.py           # Calcul des mÃ©triques (WER, CER)
â”œâ”€â”€ benchmark/                  # DonnÃ©es de benchmark
â”‚   â”œâ”€â”€ meetings/
â”‚   â”œâ”€â”€ telephone/
â”‚   â”œâ”€â”€ accents/
â”‚   â””â”€â”€ longform/
â”œâ”€â”€ data/                       # (optionnel) DonnÃ©es locales
â””â”€â”€ models/                     # (optionnel) Checkpoints
    â””â”€â”€ gilbert-whisper-l3-fr-base-v1/  # ModÃ¨le finetunÃ©
```

## ğŸ”® Prochaines Ã©tapes

- [ ] IntÃ©grer le dataset AMI pour les rÃ©unions
- [ ] ImplÃ©menter LoRA pour un fine-tuning plus efficace
- [ ] Ajouter des mÃ©triques supplÃ©mentaires (CER, BLEU)
- [ ] Support multi-GPU avec DeepSpeed
- [ ] Script d'Ã©valuation dÃ©diÃ©

## ğŸ“„ Licence

Ce projet utilise des modÃ¨les et datasets sous leurs licences respectives. VÃ©rifiez les licences avant utilisation commerciale.

## ğŸ™ Remerciements

- OpenAI pour le modÃ¨le Whisper
- HuggingFace pour les outils transformers
- Les contributeurs des datasets publics utilisÃ©s

